}
.Dendogram(word_cor, Filter_word)
.Dendogram <- function(x,y,z) {
x %>%
filter(item1 %in% y) %>%
group_by(item1) %>%
arrange(desc(item1))%>%
top_n(z) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation, fill = item1)) +
geom_col(show.legend = FALSE) +
geom_bar(stat = "identity") +
labs(x=NULL, y= NULL)+
facet_wrap(~ item1, ncol = 2, scales = "free") +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_reordered()
}
.Dendogram(word_cor2, Filter_word,5)
library(widyr)
# we need to filter for at least relatively common words first
word_cor <- par_Palavras %>%
group_by(word1) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word1,word2, sort = TRUE)
word_cor2 <- par_Palavras %>%
group_by(word2) %>%
filter(n() >= 20) %>%
widyr::pairwise_cor(word1,word2, sort = TRUE)
Filter_word <- c("trabalho", "condicoes", "organização","gestao","acoes", "riscos", "repertório", "competência", "conhecimento", "procedimentos", "desing" , "habilidades", "ambiente", "segurança", "técnicas" )
.Dendogram <- function(x,y,z) {
x %>%
filter(item1 %in% y) %>%
group_by(item1) %>%
arrange(desc(item1))%>%
top_n(z) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation, fill = item1)) +
geom_col(show.legend = FALSE) +
geom_bar(stat = "identity") +
labs(x=NULL, y= NULL)+
facet_wrap(~ item1, ncol = 2, scales = "free") +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_reordered()
}
.Dendogram(word_cor, Filter_word,10)
.Dendogram(word_cor2, Filter_word,5)
word_cor2 <- par_Palavras %>%
group_by(word2) %>%
filter(n() >= 2) %>%
widyr::pairwise_cor(word1,word2, sort = TRUE)
.Dendogram(word_cor2, Filter_word,5)
.Dendogram(word_cor2, Filter_word,10)
.Dendogram(word_cor2, Filter_word,15)
.Dendogram(word_cor2, Filter_word,2)
.Dendogram(word_cor2, Filter_word,1)
.Dendogram(word_cor2, Filter_word,3)
# remove "www" "http"
my_stopwords <- tibble(Palavra = c(as.character(1:3),
"www","http", ",", "de","os","nao","da","sao","um","dos","em","para","possui","sobre","das","na","uma","se","não", "outro", "entao", "existem", "outros", "aos", "quando", "seus", "seu", "nas", "na","evitar", "possuem", "ou", "sua", "então", "podem", "esta", "bem", "ha", "há","forem"))
par_Palavras <- TidyT %>%
unnest_tokens(Palavra, text, token = "ngrams", n = 2) %>%
mutate(Palavra = na.omit(NormalizaParaTextMining(Palavra))) %>%
anti_join(palavrasRemover) %>%
anti_join(my_stopwords) %>%
separate(Palavra, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% my_stopwords$Palavra) %>%
filter(!word2 %in% my_stopwords$Palavra) %>%
count(word1, word2, sort = TRUE)
# remove NA
par_Palavras <- na.omit(par_Palavras)
DT::datatable(par_Palavras)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
# Pacotes e Funções
library(tidyverse) # Manipulacao eficiente de dados
library(tidytext) # Manipulacao eficiente de texto
library(textreadr) # Leitura de pdf para texto
library(tm) # Pacote de mineracao de texto com stopwords
library(wordcloud) # Grafico nuvem de palavras
library(igraph)
library(ggraph)
library(ggplot2)
library(dplyr)
library(pdftools)
library(RRPP)
# Chunk 3
setwd("~/Text Mining")
# Chunk 4
# Função para normalizar texto
NormalizaParaTextMining <- function(texto){
# Normaliza texto
texto %>%
chartr(
old = "áéíóúÁÉÍÓÚýÝàèìòùÀÈÌÒÙâêîôûÂÊÎÔÛãõÃÕñÑäëïöüÄËÏÖÜÿçÇ´`^~¨:.!?&$@#0123456789",
new = "aeiouAEIOUyYaeiouAEIOUaeiouAEIOUaoAOnNaeiouAEIOUycC                       ",
x = .) %>% # Elimina acentos e caracteres desnecessarios
str_squish() %>% # Elimina espacos excedentes
tolower() %>% # Converte para minusculo
return() # Retorno da funcao
}
# Chunk 5
# Função para Palavras que podem se retiradas
# Lista de palavras para remover
palavrasRemover <- c(stopwords(kind = "pt"), letters) %>%
as.tibble() %>%
rename(Palavra = value) %>%
mutate(Palavra = NormalizaParaTextMining(Palavra))
# Chunk 6
# Arquivo pdf
arquivoPdf <- "~/Text Mining/Frameworkhf.pdf"
# Chunk 7
TidyT <- arquivoPdf %>%
read_pdf() %>%
as.tibble() %>%
select(text)
# Chunk 8
CleanW <- TidyT  %>%
unnest_tokens(Palavra, text) %>%
mutate(Palavra = NormalizaParaTextMining(Palavra)) %>%
anti_join(palavrasRemover)
# Chunk 9
frequenciaPalavras <-  CleanW  %>%
count(Palavra, sort = TRUE) %>%
filter(Palavra != "") %>%
arrange(desc(n))
# Visualiza frequencia de palavras
DT::datatable(frequenciaPalavras)
# Chunk 10
Top_15 <- arrange(frequenciaPalavras[1:15,1:2], desc(n))
# plotando as top 15 palavras.
Top_15 %>%
ggplot(aes(x = Palavra, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(y = "Contagem",
x = "Palavras únicas",
title = "Contagem de palavras únicas encontradas no Relatório")
barplot(frequenciaPalavras[1:15,1:2]$n,
names.arg=frequenciaPalavras[1:15,1:2]$Palavra,
ylab="Quantidade",
ylim = c(40, 100),
las = 2,
col=rainbow(8),
main="Top 15")
# Chunk 11
# Cria nuvem de palavras
wordcloud(
words = frequenciaPalavras$Palavra,
freq = frequenciaPalavras$n,
min.freq = 2,
max.words = 300,
random.order = FALSE,
rot.per = 0.35,
colors = brewer.pal(8, "Dark2")
)
# Chunk 12
# remove "www" "http"
my_stopwords <- tibble(Palavra = c(as.character(1:3),
"www","http", ",", "de","os","nao","da","sao","um","dos","em","para","possui","sobre","das","na","uma","se","não", "outro", "entao", "existem", "outros", "aos", "quando", "seus", "seu", "nas", "na","evitar", "possuem", "ou", "sua", "então", "podem", "esta", "bem", "ha", "há","forem"))
par_Palavras <- TidyT %>%
unnest_tokens(Palavra, text, token = "ngrams", n = 2) %>%
mutate(Palavra = na.omit(NormalizaParaTextMining(Palavra))) %>%
anti_join(palavrasRemover) %>%
anti_join(my_stopwords) %>%
separate(Palavra, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% my_stopwords$Palavra) %>%
filter(!word2 %in% my_stopwords$Palavra) %>%
count(word1, word2, sort = TRUE)
# remove NA
par_Palavras <- na.omit(par_Palavras)
DT::datatable(par_Palavras)
# Chunk 13
## Function to show word network
.Par_net = function(z) {
par_Palavras %>%
filter(n >= z) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 4) +
geom_node_text(color = "red", aes(label = name), vjust = 1.8, size=3) +
labs(title= "Word graph  - HF2 Framework",
subtitle = paste("Pairwise analysis words >= ", z, " frequency",
x = "", y = ""))
}
par(mfrow=c(2,2))
.Par_net(10)
.Par_net(5)
.Par_net(3)
# Chunk 14
trio_Palavras <- TidyT %>%
unnest_tokens(Palavra, text, token = "ngrams", n = 3) %>%
separate(Palavra, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word,
!word1 %in% my_stopwords$Palavra,
!word2 %in% my_stopwords$Palavra,
!word3 %in% my_stopwords$Palavra) %>%
count(word1, word2, word3, sort = TRUE)
DT::datatable(trio_Palavras)
# Chunk 15
library(widyr)
# we need to filter for at least relatively common words first
word_cor <- par_Palavras %>%
group_by(word1) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word1,word2, sort = TRUE)
word_cor2 <- par_Palavras %>%
group_by(word2) %>%
filter(n() >= 2) %>%
widyr::pairwise_cor(word1,word2, sort = TRUE)
# Chunk 16
Filter_word <- c("trabalho", "condicoes", "organização","gestao","acoes", "riscos", "repertório", "competência", "conhecimento", "procedimentos", "desing" , "habilidades", "ambiente", "segurança", "técnicas" )
.Dendogram <- function(x,y,z) {
x %>%
filter(item1 %in% y) %>%
group_by(item1) %>%
arrange(desc(item1))%>%
top_n(z) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation, fill = item1)) +
geom_col(show.legend = FALSE) +
geom_bar(stat = "identity") +
labs(x=NULL, y= NULL)+
facet_wrap(~ item1, ncol = 2, scales = "free") +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_reordered()
}
.Dendogram(word_cor, Filter_word,10)
.Dendogram(word_cor2, Filter_word,5)
# Chunk 17
word_cor %>%
filter(item1 %in% c("trabalho", "condicoes", "organização","gestao","acoes", "riscos", "repertório", "competência", "conhecimento", "procedimentos", "desing" , "habilidades", "ambiente", "segurança", "técnicas" )) %>%
group_by(item1) %>%
arrange(desc(item1))%>%
top_n(10) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation, fill = item1)) +
geom_col(show.legend = FALSE) +
geom_bar(stat = "identity") +
labs(x=NULL, y= NULL)+
facet_wrap(~ item1, ncol = 2, scales = "free") +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_reordered()
# Chunk 18
palavras=word_cor
wordnetwork <- head(palavras, 100)
wordnetwork <- graph_from_data_frame(wordnetwork)
wordnetwork2 <- as.undirected(wordnetwork)
comm <- cluster_fast_greedy(wordnetwork2, weights = E(wordnetwork2)$n)
plot_dendrogram(comm, main="Pairwise word clusters dendogram", cex=.9, hang=-1)
# Chunk 19
word_cor1 <- par_Palavras %>%
group_by(word1) %>%
filter(n() >= 15) %>%
pairwise_cor(word1,word2, sort = TRUE)
word_cor1 %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
library(widyr)
# we need to filter for at least relatively common words first
word_cor <- par_Palavras %>%
group_by(word1) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word1,word2, sort = TRUE)
word_cor2 <- par_Palavras %>%
group_by(word2) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word1,word2, sort = TRUE)
.Dendogram(word_cor2, Filter_word,5)
.Dendogram(word_cor2, Filter_word,6)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
# Pacotes e Funções
library(tidyverse) # Manipulacao eficiente de dados
library(tidytext) # Manipulacao eficiente de texto
library(textreadr) # Leitura de pdf para texto
library(tm) # Pacote de mineracao de texto com stopwords
library(wordcloud) # Grafico nuvem de palavras
library(igraph)
library(ggraph)
library(ggplot2)
library(dplyr)
library(pdftools)
library(RRPP)
# Chunk 3
setwd("~/Text Mining")
# Chunk 4
# Função para normalizar texto
NormalizaParaTextMining <- function(texto){
# Normaliza texto
texto %>%
chartr(
old = "áéíóúÁÉÍÓÚýÝàèìòùÀÈÌÒÙâêîôûÂÊÎÔÛãõÃÕñÑäëïöüÄËÏÖÜÿçÇ´`^~¨:.!?&$@#0123456789",
new = "aeiouAEIOUyYaeiouAEIOUaeiouAEIOUaoAOnNaeiouAEIOUycC                       ",
x = .) %>% # Elimina acentos e caracteres desnecessarios
str_squish() %>% # Elimina espacos excedentes
tolower() %>% # Converte para minusculo
return() # Retorno da funcao
}
# Chunk 5
# Função para Palavras que podem se retiradas
# Lista de palavras para remover
palavrasRemover <- c(stopwords(kind = "pt"), letters) %>%
as.tibble() %>%
rename(Palavra = value) %>%
mutate(Palavra = NormalizaParaTextMining(Palavra))
# Chunk 6
# Arquivo pdf
arquivoPdf <- "~/Text Mining/Frameworkhf.pdf"
# Chunk 7
TidyT <- arquivoPdf %>%
read_pdf() %>%
as.tibble() %>%
select(text)
# Chunk 8
CleanW <- TidyT  %>%
unnest_tokens(Palavra, text) %>%
mutate(Palavra = NormalizaParaTextMining(Palavra)) %>%
anti_join(palavrasRemover)
# Chunk 9
frequenciaPalavras <-  CleanW  %>%
count(Palavra, sort = TRUE) %>%
filter(Palavra != "") %>%
arrange(desc(n))
# Visualiza frequencia de palavras
DT::datatable(frequenciaPalavras)
# Chunk 10
Top_15 <- arrange(frequenciaPalavras[1:15,1:2], desc(n))
# plotando as top 15 palavras.
Top_15 %>%
ggplot(aes(x = Palavra, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(y = "Contagem",
x = "Palavras únicas",
title = "Contagem de palavras únicas encontradas no Relatório")
barplot(frequenciaPalavras[1:15,1:2]$n,
names.arg=frequenciaPalavras[1:15,1:2]$Palavra,
ylab="Quantidade",
ylim = c(40, 100),
las = 2,
col=rainbow(8),
main="Top 15")
# Chunk 11
# Cria nuvem de palavras
wordcloud(
words = frequenciaPalavras$Palavra,
freq = frequenciaPalavras$n,
min.freq = 2,
max.words = 300,
random.order = FALSE,
rot.per = 0.35,
colors = brewer.pal(8, "Dark2")
)
# Chunk 12
# remove "www" "http"
my_stopwords <- tibble(Palavra = c(as.character(1:3),
"www","http", ",", "de","os","nao","da","sao","um","dos","em","para","possui","sobre","das","na","uma","se","não", "outro", "entao", "existem", "outros", "aos", "quando", "seus", "seu", "nas", "na","evitar", "possuem", "ou", "sua", "então", "podem", "esta", "bem", "ha", "há","forem", "documentadas", "documentados", "alta"))
par_Palavras <- TidyT %>%
unnest_tokens(Palavra, text, token = "ngrams", n = 2) %>%
mutate(Palavra = na.omit(NormalizaParaTextMining(Palavra))) %>%
anti_join(palavrasRemover) %>%
anti_join(my_stopwords) %>%
separate(Palavra, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% my_stopwords$Palavra) %>%
filter(!word2 %in% my_stopwords$Palavra) %>%
count(word1, word2, sort = TRUE)
# remove NA
par_Palavras <- na.omit(par_Palavras)
DT::datatable(par_Palavras)
# Chunk 13
## Function to show word network
.Par_net = function(z) {
par_Palavras %>%
filter(n >= z) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 4) +
geom_node_text(color = "red", aes(label = name), vjust = 1.8, size=3) +
labs(title= "Word graph  - HF2 Framework",
subtitle = paste("Pairwise analysis words >= ", z, " frequency",
x = "", y = ""))
}
par(mfrow=c(2,2))
.Par_net(10)
.Par_net(5)
.Par_net(3)
# Chunk 14
trio_Palavras <- TidyT %>%
unnest_tokens(Palavra, text, token = "ngrams", n = 3) %>%
separate(Palavra, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word,
!word1 %in% my_stopwords$Palavra,
!word2 %in% my_stopwords$Palavra,
!word3 %in% my_stopwords$Palavra) %>%
count(word1, word2, word3, sort = TRUE)
DT::datatable(trio_Palavras)
# Chunk 15
library(widyr)
# we need to filter for at least relatively common words first
word_cor <- par_Palavras %>%
group_by(word1) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word1,word2, sort = TRUE)
word_cor2 <- par_Palavras %>%
group_by(word2) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word1,word2, sort = TRUE)
# Chunk 16
Filter_word <- c("trabalho", "condicoes", "organização","gestao","acoes", "riscos", "repertório", "competência", "conhecimento", "procedimentos", "desing" , "habilidades", "ambiente", "segurança", "técnicas" )
.Dendogram <- function(x,y,z) {
x %>%
filter(item1 %in% y) %>%
group_by(item1) %>%
arrange(desc(item1))%>%
top_n(z) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation, fill = item1)) +
geom_col(show.legend = FALSE) +
geom_bar(stat = "identity") +
labs(x=NULL, y= NULL)+
facet_wrap(~ item1, ncol = 2, scales = "free") +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_reordered()
}
.Dendogram(word_cor, Filter_word,10)
.Dendogram(word_cor2, Filter_word,6)
# Chunk 17
word_cor %>%
filter(item1 %in% c("trabalho", "condicoes", "organização","gestao","acoes", "riscos", "repertório", "competência", "conhecimento", "procedimentos", "desing" , "habilidades", "ambiente", "segurança", "técnicas" )) %>%
group_by(item1) %>%
arrange(desc(item1))%>%
top_n(10) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation, fill = item1)) +
geom_col(show.legend = FALSE) +
geom_bar(stat = "identity") +
labs(x=NULL, y= NULL)+
facet_wrap(~ item1, ncol = 2, scales = "free") +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_reordered()
# Chunk 18
palavras=word_cor
wordnetwork <- head(palavras, 100)
wordnetwork <- graph_from_data_frame(wordnetwork)
wordnetwork2 <- as.undirected(wordnetwork)
comm <- cluster_fast_greedy(wordnetwork2, weights = E(wordnetwork2)$n)
plot_dendrogram(comm, main="Pairwise word clusters dendogram", cex=.9, hang=-1)
# Chunk 19
word_cor1 <- par_Palavras %>%
group_by(word1) %>%
filter(n() >= 15) %>%
pairwise_cor(word1,word2, sort = TRUE)
word_cor1 %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cor1 <- par_Palavras %>%
group_by(word1) %>%
filter(n() >= 15) %>%
pairwise_cor(word1,word2, sort = TRUE)
word_cor3 <- par_Palavras %>%
group_by(word1) %>%
filter(n() >= 20) %>%
pairwise_cor(word1,word2, sort = TRUE)
word_cor1 %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cor3 %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
